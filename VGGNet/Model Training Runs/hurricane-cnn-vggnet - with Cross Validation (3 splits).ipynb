{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.10","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Imports","metadata":{"execution":{"iopub.status.busy":"2021-08-13T17:24:20.382233Z","iopub.execute_input":"2021-08-13T17:24:20.382698Z","iopub.status.idle":"2021-08-13T17:24:20.387768Z","shell.execute_reply.started":"2021-08-13T17:24:20.382668Z","shell.execute_reply":"2021-08-13T17:24:20.386839Z"}}},{"cell_type":"code","source":"import numpy as np\nimport glob\nfrom PIL import Image\nimport tensorflow as tf\nimport matplotlib.pyplot as plt\nfrom tensorflow import keras\nfrom tensorflow.keras import layers\nfrom tensorflow.keras import regularizers\nfrom tensorflow.keras import optimizers\nfrom keras.datasets import cifar10\nfrom keras.preprocessing.image import ImageDataGenerator\nfrom sklearn.metrics import confusion_matrix\nfrom sklearn.model_selection import KFold\nimport pandas as pd\nimport seaborn as sn\nsn.set(font_scale=1.4) # for label size","metadata":{"execution":{"iopub.status.busy":"2021-08-18T14:03:26.294497Z","iopub.execute_input":"2021-08-18T14:03:26.295004Z","iopub.status.idle":"2021-08-18T14:03:32.711566Z","shell.execute_reply.started":"2021-08-18T14:03:26.294960Z","shell.execute_reply":"2021-08-18T14:03:32.710542Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# Load DataSet\n\n0 means unlikely to have flood damage, 1 means likely to have flood damage\n\n[0,1] --> damage\n\n[1,0] --> no damage","metadata":{}},{"cell_type":"markdown","source":"## Load Training Set","metadata":{}},{"cell_type":"code","source":"images = glob.glob('/kaggle/input/satellite-images-of-hurricane-damage/train_another/damage/*.jpeg')\ndmg_img = []\nfor i in images:\n  im = Image.open(i)\n  im_arr = np.array(im)\n  dmg_img.append(im_arr)\ndmg_img = np.array(dmg_img)\n\nimages = glob.glob('/kaggle/input/satellite-images-of-hurricane-damage/train_another/no_damage/*.jpeg')\nno_dmg_img = []\nfor i in images:\n  im = Image.open(i)\n  im_arr = np.array(im)\n  no_dmg_img.append(im_arr)\nno_dmg_img = np.array(no_dmg_img)\n\nX_train = np.concatenate((dmg_img, no_dmg_img))\ndmg_img_y = np.array(dmg_img.shape[0] * [[0,1]])\nno_dmg_img_y = np.array(no_dmg_img.shape[0] * [[1,0]])\ny_train = np.concatenate((dmg_img_y, no_dmg_img_y))\n\n# shuffle arrays so that the array is a mix of damage and no_damage in a random order\nshuffler = np.random.permutation(X_train.shape[0])\nX_train = X_train[shuffler]\ny_train = y_train[shuffler]","metadata":{"execution":{"iopub.status.busy":"2021-08-18T14:04:01.683314Z","iopub.execute_input":"2021-08-18T14:04:01.683674Z","iopub.status.idle":"2021-08-18T14:04:28.623423Z","shell.execute_reply.started":"2021-08-18T14:04:01.683643Z","shell.execute_reply":"2021-08-18T14:04:28.622553Z"},"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"## Load Validation Set","metadata":{}},{"cell_type":"code","source":"images = glob.glob('/kaggle/input/satellite-images-of-hurricane-damage/validation_another/damage/*.jpeg')\ndmg_img = []\nfor i in images:\n  im = Image.open(i)\n  im_arr = np.array(im)\n  dmg_img.append(im_arr)\ndmg_img = np.array(dmg_img)\n\nimages = glob.glob('/kaggle/input/satellite-images-of-hurricane-damage/validation_another/no_damage/*.jpeg')\nno_dmg_img = []\nfor i in images:\n  im = Image.open(i)\n  im_arr = np.array(im)\n  no_dmg_img.append(im_arr)\nno_dmg_img = np.array(no_dmg_img)\n\nX_val = np.concatenate((dmg_img, no_dmg_img))\ndmg_img_y = np.array(dmg_img.shape[0] * [[0,1]])\nno_dmg_img_y = np.array(no_dmg_img.shape[0] * [[1,0]])\ny_val = np.concatenate((dmg_img_y, no_dmg_img_y))\n\n# shuffle arrays so that the array is a mix of damage and no_damage in a random order\nshuffler = np.random.permutation(X_val.shape[0])\nX_val = X_val[shuffler]\ny_val = y_val[shuffler]","metadata":{"execution":{"iopub.status.busy":"2021-08-18T14:04:28.624785Z","iopub.execute_input":"2021-08-18T14:04:28.625290Z","iopub.status.idle":"2021-08-18T14:04:33.719584Z","shell.execute_reply.started":"2021-08-18T14:04:28.625257Z","shell.execute_reply":"2021-08-18T14:04:33.718664Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# combine training and validation data - for cross-validation\nX_all = np.concatenate((X_train, X_val))\ny_all = np.concatenate((y_train, y_val))","metadata":{"execution":{"iopub.status.busy":"2021-08-18T14:04:33.720999Z","iopub.execute_input":"2021-08-18T14:04:33.721411Z","iopub.status.idle":"2021-08-18T14:04:33.952626Z","shell.execute_reply.started":"2021-08-18T14:04:33.721381Z","shell.execute_reply":"2021-08-18T14:04:33.951838Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"## Load Test Set","metadata":{}},{"cell_type":"code","source":"images = glob.glob('/kaggle/input/satellite-images-of-hurricane-damage/test/damage/*.jpeg')\ndmg_img = []\nfor i in images:\n  im = Image.open(i)\n  im_arr = np.array(im)\n  dmg_img.append(im_arr)\ndmg_img = np.array(dmg_img)\n\nimages = glob.glob('/kaggle/input/satellite-images-of-hurricane-damage/test/no_damage/*.jpeg')\nno_dmg_img = []\nfor i in images:\n  im = Image.open(i)\n  im_arr = np.array(im)\n  no_dmg_img.append(im_arr)\nno_dmg_img = np.array(no_dmg_img)\n\nX_test = np.concatenate((dmg_img, no_dmg_img))\ndmg_img_y = np.array(dmg_img.shape[0] * [[0,1]])\nno_dmg_img_y = np.array(no_dmg_img.shape[0] * [[1,0]])\ny_test = np.concatenate((dmg_img_y, no_dmg_img_y))\n\n# shuffle arrays so that the array is a mix of damage and no_damage in a random order\nshuffler = np.random.permutation(X_val.shape[0])\nX_test = X_test[shuffler]\ny_test = y_test[shuffler]","metadata":{"execution":{"iopub.status.busy":"2021-08-18T14:04:33.953889Z","iopub.execute_input":"2021-08-18T14:04:33.954310Z","iopub.status.idle":"2021-08-18T14:04:38.938401Z","shell.execute_reply.started":"2021-08-18T14:04:33.954280Z","shell.execute_reply":"2021-08-18T14:04:38.937392Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"markdown","source":"# Pre-Process DataSet","metadata":{}},{"cell_type":"code","source":"# pre-process data \ndef scale_pixels(train, val, test, X_all):\n\t# convert from integers to floats and normalize between 0-1\n  train_norm = train.astype('float32') / 255.0\n  val_norm = val.astype('float32') / 255.0\n  test_norm = test.astype('float32') / 255.0\n  X_all_norm = X_all.astype('float32') / 255.0\n\t# return normalized images\n  return train_norm, val_norm, test_norm, X_all_norm\n\nX_train, X_val, X_test, X_all = scale_pixels(X_train, X_val, X_test, X_all)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T14:04:38.940024Z","iopub.execute_input":"2021-08-18T14:04:38.940336Z","iopub.status.idle":"2021-08-18T14:04:41.368008Z","shell.execute_reply.started":"2021-08-18T14:04:38.940307Z","shell.execute_reply":"2021-08-18T14:04:41.364637Z"},"trusted":true},"execution_count":13,"outputs":[]},{"cell_type":"markdown","source":"# Plot Diagnostics","metadata":{}},{"cell_type":"code","source":"# plot diagnostic learning curves\ndef summarize_diagnostics(history):\n  # blue = training data; orange = validation data\n\t# plot validation loss\n\tplt.subplot(211)\n\tplt.title('Cross Entropy Loss')\n\tplt.plot(history.history['loss'], color='blue', label='train')\n\tplt.plot(history.history['val_loss'], color='orange', label='validation')\n\t# plot accuracy\n\tplt.subplot(212)\n\tplt.title('Classification Accuracy')\n\tplt.plot(history.history['accuracy'], color='blue', label='train')\n\tplt.plot(history.history['val_accuracy'], color='orange', label='validation')","metadata":{"execution":{"iopub.status.busy":"2021-08-18T14:04:41.369668Z","iopub.execute_input":"2021-08-18T14:04:41.369951Z","iopub.status.idle":"2021-08-18T14:04:41.377745Z","shell.execute_reply.started":"2021-08-18T14:04:41.369923Z","shell.execute_reply":"2021-08-18T14:04:41.375936Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"# Create CNN Model","metadata":{}},{"cell_type":"code","source":"# define cnn model\ndef define_model(learning_rate):\n  model = keras.Sequential()\n  model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(128, 128, 3)))\n  model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Flatten())\n  model.add(layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n  model.add(layers.Dense(2, activation='softmax'))  # compile model\n  opt = optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n  model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n  return model","metadata":{"execution":{"iopub.status.busy":"2021-08-18T14:04:41.379831Z","iopub.execute_input":"2021-08-18T14:04:41.380136Z","iopub.status.idle":"2021-08-18T14:04:41.393842Z","shell.execute_reply.started":"2021-08-18T14:04:41.380108Z","shell.execute_reply":"2021-08-18T14:04:41.392485Z"},"trusted":true},"execution_count":15,"outputs":[]},{"cell_type":"markdown","source":"## CNN Model with Batch Normalization","metadata":{}},{"cell_type":"code","source":"# define cnn model\ndef define_model_batch_normalization(learning_rate):\n  model = keras.Sequential()\n  model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(128, 128, 3)))\n  model.add(layers.BatchNormalization())\n  model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.BatchNormalization())\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.BatchNormalization())\n  model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.BatchNormalization())\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.BatchNormalization())\n  model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.BatchNormalization())\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Flatten())\n  model.add(layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n  model.add(layers.BatchNormalization())\n  model.add(layers.Dense(2, activation='softmax'))  # compile model\n  opt = optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n  model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n  return model","metadata":{"execution":{"iopub.status.busy":"2021-08-18T01:29:56.472436Z","iopub.status.idle":"2021-08-18T01:29:56.472962Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN Model with Dropout Regularization","metadata":{}},{"cell_type":"code","source":"# define cnn model\ndef define_model_dropout_regularization(learning_rate):\n  model = keras.Sequential()\n  model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(128, 128, 3)))\n  model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Dropout(0.2)) # retain 80% of neurons\n  model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Dropout(0.3))\n  model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Dropout(0.4))\n  model.add(layers.Flatten())\n  model.add(layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n  model.add(layers.Dropout(0.5))\n  model.add(layers.Dense(2, activation='softmax'))  # compile model\n  opt = optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n  model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n  return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN Model with Dropout Regularization and Batch Normalization*","metadata":{}},{"cell_type":"code","source":"# define cnn model\ndef define_model_dropout_regularization_batch_normalization(learning_rate):\n  model = keras.Sequential()\n  model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', input_shape=(128, 128, 3)))\n  model.add(layers.BatchNormalization())\n  model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.BatchNormalization())\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Dropout(0.2)) # retain 80% of neurons\n  model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.BatchNormalization())\n  model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.BatchNormalization())\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Dropout(0.3))\n  model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.BatchNormalization())\n  model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same'))\n  model.add(layers.BatchNormalization())\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Dropout(0.4))\n  model.add(layers.Flatten())\n  model.add(layers.Dense(128, activation='relu', kernel_initializer='he_uniform'))\n  model.add(layers.BatchNormalization())\n  model.add(layers.Dropout(0.5))\n  model.add(layers.Dense(2, activation='softmax'))  # compile model\n  opt = optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n  model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n  return model","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:22:53.352597Z","iopub.execute_input":"2021-08-17T14:22:53.352919Z","iopub.status.idle":"2021-08-17T14:22:53.365756Z","shell.execute_reply.started":"2021-08-17T14:22:53.352887Z","shell.execute_reply":"2021-08-17T14:22:53.364727Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## CNN Model with Weight Regularization","metadata":{}},{"cell_type":"code","source":"# define cnn model\ndef define_model_weight_regularization(learning_rate):\n  model = keras.Sequential()\n  model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(0.01), input_shape=(128, 128, 3)))\n  model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(0.01)))\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(0.01)))\n  model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(0.01)))\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(0.01)))\n  model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(0.01)))\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Flatten())\n  model.add(layers.Dense(128, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.01)))\n  model.add(layers.Dense(2, activation='softmax'))  # compile model\n  opt = optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n  model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n  return model","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### CNN Model with Weight Regularization and Batch Normalization","metadata":{}},{"cell_type":"code","source":"# define cnn model\ndef define_model_weight_regularization_batch_normalization(learning_rate):\n  model = keras.Sequential()\n  model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(0.01), input_shape=(128, 128, 3)))\n  model.add(layers.BatchNormalization())\n  model.add(layers.Conv2D(32, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(0.01)))\n  model.add(layers.BatchNormalization())\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(0.01)))\n  model.add(layers.BatchNormalization())\n  model.add(layers.Conv2D(64, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(0.01)))\n  model.add(layers.BatchNormalization())\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(0.01)))\n  model.add(layers.BatchNormalization())\n  model.add(layers.Conv2D(128, (3, 3), activation='relu', kernel_initializer='he_uniform', padding='same', kernel_regularizer=regularizers.l2(0.01)))\n  model.add(layers.BatchNormalization())\n  model.add(layers.MaxPooling2D((2, 2)))\n  model.add(layers.Flatten())\n  model.add(layers.Dense(128, activation='relu', kernel_initializer='he_uniform', kernel_regularizer=regularizers.l2(0.01)))\n  model.add(layers.BatchNormalization())\n  model.add(layers.Dense(2, activation='softmax'))  # compile model\n  opt = optimizers.SGD(learning_rate=learning_rate, momentum=0.9)\n  model.compile(optimizer=opt, loss='binary_crossentropy', metrics=['accuracy'])\n  return model","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:22:53.367349Z","iopub.execute_input":"2021-08-17T14:22:53.3677Z","iopub.status.idle":"2021-08-17T14:22:53.382088Z","shell.execute_reply.started":"2021-08-17T14:22:53.367667Z","shell.execute_reply":"2021-08-17T14:22:53.381079Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Train Data\nHyperparameters are inputted into the function","metadata":{}},{"cell_type":"code","source":"def train(X_train, X_val, X_test, y_train, y_val, y_test, learning_rate, epochs):\n  # define model\n  model = define_model(learning_rate)\n  # define early stopping\n  es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', min_delta=0, patience=5, verbose=1)\n  # fit model\n  history = model.fit(X_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_val, y_val), verbose=1, callbacks=[es])\n  # plot model diagnostics\n  summarize_diagnostics(history)\n  # evaluate model\n  _, accuracy = model.evaluate(X_test, y_test, verbose=1)\n  return model, accuracy","metadata":{"execution":{"iopub.status.busy":"2021-08-18T14:04:41.396116Z","iopub.execute_input":"2021-08-18T14:04:41.396415Z","iopub.status.idle":"2021-08-18T14:04:41.410594Z","shell.execute_reply.started":"2021-08-18T14:04:41.396380Z","shell.execute_reply":"2021-08-18T14:04:41.409431Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"model, accuracy = train(X_train, X_val, X_test, y_train, y_val, y_test, 0.001, 75)\nprint(accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-08-18T01:38:38.114163Z","iopub.execute_input":"2021-08-18T01:38:38.114586Z","iopub.status.idle":"2021-08-18T01:38:46.195296Z","shell.execute_reply.started":"2021-08-18T01:38:38.11454Z","shell.execute_reply":"2021-08-18T01:38:46.192719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_test, accuracy = train(X_train, X_val, X_test, y_train, y_val, y_test, 0.001, 75)\nprint(accuracy)\ny_pred = model_test.predict(X_test)\ny_pred_val = y_pred.argmax(axis=1)\ny_test_val = y_test.argmax(axis=1)\ntn, fp, fn, tp = confusion_matrix(y_test_val, y_pred_val).ravel()\nprint(\"{} {} {} {}\".format(tn, fp, fn, tp))\nprint(\"TPR/Recall: {}\".format(tp/(tp+fn)))\nprint(\"TNR/Specificity: {}\".format(tn/(tn+fp)))\nprint(\"PPV/Precision: {}\".format(tp/(tp+fp)))\nprint(\"NPV: {}\".format(tn/(tn+fn)))","metadata":{"execution":{"iopub.status.busy":"2021-08-15T18:17:36.414439Z","iopub.execute_input":"2021-08-15T18:17:36.414855Z","iopub.status.idle":"2021-08-15T22:54:08.378534Z","shell.execute_reply.started":"2021-08-15T18:17:36.414813Z","shell.execute_reply":"2021-08-15T22:54:08.376979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Data w/ Cross Validation","metadata":{}},{"cell_type":"code","source":"kf = KFold(n_splits=3)\naccuracy_list = []\nconfusion_matrix_list = []\nTPR_list, TNR_list, PPV_list, NPV_list = [], [], [], []\nfor tr, val in kf.split(X_all):\n  X_train_cv = X_all[tr]\n  X_val_cv = X_all[val]\n  y_train_cv = y_all[tr]\n  y_val_cv = y_all[val]\n  model, accuracy = train(X_train_cv, X_val_cv, X_test, y_train_cv, y_val_cv, y_test, 0.001, 75)\n  print(accuracy)\n  accuracy_list.append(accuracy)\n  # compute confusion matrix \n  y_pred = model.predict(X_test)\n  y_pred_val = y_pred.argmax(axis=1)\n  y_test_val = y_test.argmax(axis=1)\n  confusion_matrix_list.append(confusion_matrix(y_test_val, y_pred_val))\n  tn, fp, fn, tp = confusion_matrix_list[-1].ravel()\n  # calculate tpr, tnr, ppv, and npv, and add them to the lists\n  TPR_list.append(tp/(tp+fn))\n  TNR_list.append(tn/(tn+fp))\n  PPV_list.append(tp/(tp+fp))\n  NPV_list.append(tn/(tn+fn))\n  print(\"{} {} {} {}\".format(tn, fp, fn, tp))\n  print(\"TPR/Recall: {}\".format(tp/(tp+fn)))\n  print(\"TNR/Specificity: {}\".format(tn/(tn+fp)))\n  print(\"PPV/Precision: {}\".format(tp/(tp+fp)))\n  print(\"NPV: {}\".format(tn/(tn+fn)))\n\n# calculate mean and standard deviation for each list\naccuracy_mean = np.mean(accuracy_list)\naccuracy_std = np.std(accuracy_list)\nTPR_mean = np.mean(TPR_list)\nTPR_std = np.std(TPR_list)\nTNR_mean = np.mean(TNR_list)\nTNR_std = np.std(TNR_list)\nPPV_mean = np.mean(PPV_list)\nPPV_std = np.std(PPV_list)\nNPV_mean = np.mean(NPV_list)\nNPV_std = np.std(NPV_list)\n\n# total confusion matrix summed together\nconfusion_matrix_comb = sum(confusion_matrix_list)\ndf = pd.DataFrame(confusion_matrix_comb, index = ['F', 'T'], columns = ['F', 'T'])\nsn.heatmap(df, annot=True) # create a heatmap","metadata":{"execution":{"iopub.status.busy":"2021-08-18T14:04:41.412240Z","iopub.execute_input":"2021-08-18T14:04:41.412564Z"},"trusted":true},"execution_count":null,"outputs":[{"name":"stdout","text":"Epoch 1/75\n125/125 [==============================] - 488s 4s/step - loss: 0.6987 - accuracy: 0.5301 - val_loss: 0.6121 - val_accuracy: 0.7870\nEpoch 2/75\n125/125 [==============================] - 480s 4s/step - loss: 0.5696 - accuracy: 0.7631 - val_loss: 0.5107 - val_accuracy: 0.7755\nEpoch 3/75\n125/125 [==============================] - 476s 4s/step - loss: 0.5061 - accuracy: 0.7747 - val_loss: 0.4561 - val_accuracy: 0.8388\nEpoch 4/75\n125/125 [==============================] - 476s 4s/step - loss: 0.4259 - accuracy: 0.8296 - val_loss: 0.3524 - val_accuracy: 0.8698\nEpoch 5/75\n125/125 [==============================] - 477s 4s/step - loss: 0.3600 - accuracy: 0.8651 - val_loss: 0.3996 - val_accuracy: 0.8382\nEpoch 6/75\n125/125 [==============================] - 475s 4s/step - loss: 0.3509 - accuracy: 0.8683 - val_loss: 0.3029 - val_accuracy: 0.8878\nEpoch 7/75\n125/125 [==============================] - 477s 4s/step - loss: 0.3088 - accuracy: 0.8792 - val_loss: 0.3054 - val_accuracy: 0.8825\nEpoch 8/75\n125/125 [==============================] - 478s 4s/step - loss: 0.2922 - accuracy: 0.8874 - val_loss: 0.2553 - val_accuracy: 0.8972\nEpoch 9/75\n125/125 [==============================] - 478s 4s/step - loss: 0.2615 - accuracy: 0.8973 - val_loss: 0.2919 - val_accuracy: 0.8842\nEpoch 10/75\n125/125 [==============================] - 477s 4s/step - loss: 0.2567 - accuracy: 0.8875 - val_loss: 0.2548 - val_accuracy: 0.8945\nEpoch 11/75\n125/125 [==============================] - 483s 4s/step - loss: 0.2239 - accuracy: 0.9087 - val_loss: 0.2393 - val_accuracy: 0.8978\nEpoch 12/75\n125/125 [==============================] - 482s 4s/step - loss: 0.1982 - accuracy: 0.9181 - val_loss: 0.1716 - val_accuracy: 0.9283\nEpoch 13/75\n125/125 [==============================] - 479s 4s/step - loss: 0.1656 - accuracy: 0.9357 - val_loss: 0.2901 - val_accuracy: 0.8935\nEpoch 14/75\n125/125 [==============================] - 475s 4s/step - loss: 0.2226 - accuracy: 0.9109 - val_loss: 0.1716 - val_accuracy: 0.9245\nEpoch 15/75\n125/125 [==============================] - 478s 4s/step - loss: 0.1615 - accuracy: 0.9340 - val_loss: 0.1726 - val_accuracy: 0.9215\nEpoch 16/75\n125/125 [==============================] - 479s 4s/step - loss: 0.1494 - accuracy: 0.9376 - val_loss: 0.2115 - val_accuracy: 0.9097\nEpoch 17/75\n125/125 [==============================] - 479s 4s/step - loss: 0.1178 - accuracy: 0.9528 - val_loss: 0.1483 - val_accuracy: 0.9433\nEpoch 18/75\n125/125 [==============================] - 476s 4s/step - loss: 0.1186 - accuracy: 0.9533 - val_loss: 0.1178 - val_accuracy: 0.9557\nEpoch 19/75\n125/125 [==============================] - 478s 4s/step - loss: 0.1171 - accuracy: 0.9539 - val_loss: 0.1318 - val_accuracy: 0.9452\nEpoch 20/75\n125/125 [==============================] - 477s 4s/step - loss: 0.1143 - accuracy: 0.9549 - val_loss: 0.1046 - val_accuracy: 0.9595\nEpoch 21/75\n125/125 [==============================] - 480s 4s/step - loss: 0.1040 - accuracy: 0.9613 - val_loss: 0.1252 - val_accuracy: 0.9530\nEpoch 22/75\n125/125 [==============================] - 477s 4s/step - loss: 0.0973 - accuracy: 0.9626 - val_loss: 0.1033 - val_accuracy: 0.9532\nEpoch 23/75\n125/125 [==============================] - 480s 4s/step - loss: 0.0953 - accuracy: 0.9600 - val_loss: 0.1045 - val_accuracy: 0.9560\nEpoch 24/75\n125/125 [==============================] - 474s 4s/step - loss: 0.0836 - accuracy: 0.9678 - val_loss: 0.1145 - val_accuracy: 0.9575\nEpoch 25/75\n125/125 [==============================] - 473s 4s/step - loss: 0.0882 - accuracy: 0.9689 - val_loss: 0.0971 - val_accuracy: 0.9613\nEpoch 26/75\n125/125 [==============================] - 475s 4s/step - loss: 0.0813 - accuracy: 0.9694 - val_loss: 0.1015 - val_accuracy: 0.9597\nEpoch 27/75\n125/125 [==============================] - 474s 4s/step - loss: 0.0853 - accuracy: 0.9665 - val_loss: 0.1045 - val_accuracy: 0.9578\nEpoch 28/75\n125/125 [==============================] - 476s 4s/step - loss: 0.0766 - accuracy: 0.9724 - val_loss: 0.1031 - val_accuracy: 0.9607\nEpoch 29/75\n125/125 [==============================] - 471s 4s/step - loss: 0.0718 - accuracy: 0.9741 - val_loss: 0.1231 - val_accuracy: 0.9520\nEpoch 30/75\n125/125 [==============================] - 472s 4s/step - loss: 0.0750 - accuracy: 0.9732 - val_loss: 0.1574 - val_accuracy: 0.9438\nEpoch 00030: early stopping\n63/63 [==============================] - 23s 365ms/step - loss: 0.1589 - accuracy: 0.9420\n0.9419999718666077\n989 11 105 895\nTPR/Recall: 0.895\nTNR/Specificity: 0.989\nPPV/Precision: 0.9878587196467992\nNPV: 0.9040219378427788\nEpoch 1/75\n125/125 [==============================] - 473s 4s/step - loss: 0.6845 - accuracy: 0.5596 - val_loss: 0.5443 - val_accuracy: 0.7178\nEpoch 2/75\n125/125 [==============================] - 470s 4s/step - loss: 0.5414 - accuracy: 0.7558 - val_loss: 0.4355 - val_accuracy: 0.8367\nEpoch 3/75\n125/125 [==============================] - 471s 4s/step - loss: 0.4670 - accuracy: 0.8056 - val_loss: 0.4156 - val_accuracy: 0.8435\nEpoch 4/75\n125/125 [==============================] - 471s 4s/step - loss: 0.3936 - accuracy: 0.8440 - val_loss: 0.4759 - val_accuracy: 0.8083\nEpoch 5/75\n125/125 [==============================] - 470s 4s/step - loss: 0.3612 - accuracy: 0.8558 - val_loss: 0.3532 - val_accuracy: 0.8618\nEpoch 6/75\n125/125 [==============================] - 468s 4s/step - loss: 0.3141 - accuracy: 0.8751 - val_loss: 0.3086 - val_accuracy: 0.8820\nEpoch 7/75\n125/125 [==============================] - 469s 4s/step - loss: 0.2881 - accuracy: 0.8822 - val_loss: 0.3036 - val_accuracy: 0.8830\nEpoch 8/75\n125/125 [==============================] - 469s 4s/step - loss: 0.2673 - accuracy: 0.8940 - val_loss: 0.2665 - val_accuracy: 0.8950\nEpoch 9/75\n125/125 [==============================] - 470s 4s/step - loss: 0.2495 - accuracy: 0.9033 - val_loss: 0.2499 - val_accuracy: 0.8965\nEpoch 10/75\n125/125 [==============================] - 470s 4s/step - loss: 0.2214 - accuracy: 0.9096 - val_loss: 0.2687 - val_accuracy: 0.8947\nEpoch 11/75\n125/125 [==============================] - 469s 4s/step - loss: 0.2141 - accuracy: 0.9133 - val_loss: 0.2975 - val_accuracy: 0.8890\nEpoch 12/75\n125/125 [==============================] - 469s 4s/step - loss: 0.2247 - accuracy: 0.9103 - val_loss: 0.2077 - val_accuracy: 0.9143\nEpoch 13/75\n125/125 [==============================] - 467s 4s/step - loss: 0.1972 - accuracy: 0.9185 - val_loss: 0.2082 - val_accuracy: 0.9145\nEpoch 14/75\n125/125 [==============================] - 467s 4s/step - loss: 0.1759 - accuracy: 0.9294 - val_loss: 0.1864 - val_accuracy: 0.9235\nEpoch 15/75\n125/125 [==============================] - 468s 4s/step - loss: 0.1598 - accuracy: 0.9331 - val_loss: 0.1862 - val_accuracy: 0.9255\nEpoch 16/75\n125/125 [==============================] - 470s 4s/step - loss: 0.1323 - accuracy: 0.9460 - val_loss: 0.1656 - val_accuracy: 0.9310\nEpoch 17/75\n125/125 [==============================] - 469s 4s/step - loss: 0.1424 - accuracy: 0.9412 - val_loss: 0.1632 - val_accuracy: 0.9350\nEpoch 18/75\n125/125 [==============================] - 469s 4s/step - loss: 0.1322 - accuracy: 0.9446 - val_loss: 0.1627 - val_accuracy: 0.9388\nEpoch 19/75\n125/125 [==============================] - 468s 4s/step - loss: 0.0982 - accuracy: 0.9632 - val_loss: 0.1412 - val_accuracy: 0.9452\nEpoch 20/75\n125/125 [==============================] - 467s 4s/step - loss: 0.1185 - accuracy: 0.9532 - val_loss: 0.1340 - val_accuracy: 0.9482\nEpoch 21/75\n125/125 [==============================] - 467s 4s/step - loss: 0.0938 - accuracy: 0.9615 - val_loss: 0.1662 - val_accuracy: 0.9392\nEpoch 22/75\n125/125 [==============================] - 466s 4s/step - loss: 0.0937 - accuracy: 0.9617 - val_loss: 0.1904 - val_accuracy: 0.9308\nEpoch 23/75\n125/125 [==============================] - 469s 4s/step - loss: 0.0935 - accuracy: 0.9647 - val_loss: 0.1426 - val_accuracy: 0.9488\nEpoch 24/75\n125/125 [==============================] - 467s 4s/step - loss: 0.0807 - accuracy: 0.9727 - val_loss: 0.1359 - val_accuracy: 0.9452\nEpoch 25/75\n125/125 [==============================] - 468s 4s/step - loss: 0.0755 - accuracy: 0.9716 - val_loss: 0.1328 - val_accuracy: 0.9482\nEpoch 26/75\n125/125 [==============================] - 467s 4s/step - loss: 0.0723 - accuracy: 0.9757 - val_loss: 0.1604 - val_accuracy: 0.9475\nEpoch 27/75\n125/125 [==============================] - 466s 4s/step - loss: 0.0627 - accuracy: 0.9767 - val_loss: 0.1233 - val_accuracy: 0.9530\nEpoch 28/75\n125/125 [==============================] - 466s 4s/step - loss: 0.0704 - accuracy: 0.9742 - val_loss: 0.1227 - val_accuracy: 0.9548\nEpoch 29/75\n125/125 [==============================] - 465s 4s/step - loss: 0.0536 - accuracy: 0.9801 - val_loss: 0.1406 - val_accuracy: 0.9517\nEpoch 30/75\n125/125 [==============================] - 466s 4s/step - loss: 0.0696 - accuracy: 0.9774 - val_loss: 0.1270 - val_accuracy: 0.9523\nEpoch 31/75\n125/125 [==============================] - 467s 4s/step - loss: 0.0476 - accuracy: 0.9864 - val_loss: 0.1455 - val_accuracy: 0.9488\nEpoch 32/75\n125/125 [==============================] - 465s 4s/step - loss: 0.0472 - accuracy: 0.9817 - val_loss: 0.1293 - val_accuracy: 0.9563\nEpoch 33/75\n125/125 [==============================] - 466s 4s/step - loss: 0.0433 - accuracy: 0.9843 - val_loss: 0.1262 - val_accuracy: 0.9572\nEpoch 00033: early stopping\n63/63 [==============================] - 23s 364ms/step - loss: 0.0890 - accuracy: 0.9725\n0.9725000262260437\n978 22 33 967\nTPR/Recall: 0.967\nTNR/Specificity: 0.978\nPPV/Precision: 0.9777553083923155\nNPV: 0.9673590504451038\nEpoch 1/75\n125/125 [==============================] - 468s 4s/step - loss: 0.6783 - accuracy: 0.5686 - val_loss: 0.5780 - val_accuracy: 0.7030\nEpoch 2/75\n125/125 [==============================] - 470s 4s/step - loss: 0.5355 - accuracy: 0.7507 - val_loss: 0.4033 - val_accuracy: 0.8515\nEpoch 3/75\n125/125 [==============================] - 469s 4s/step - loss: 0.4615 - accuracy: 0.8109 - val_loss: 0.4226 - val_accuracy: 0.8305\nEpoch 4/75\n125/125 [==============================] - 471s 4s/step - loss: 0.3769 - accuracy: 0.8568 - val_loss: 0.3210 - val_accuracy: 0.8790\nEpoch 5/75\n113/125 [==========================>...] - ETA: 40s - loss: 0.3640 - accuracy: 0.8641","output_type":"stream"}]},{"cell_type":"code","source":"# normalized confusion matrix\nconfusion_matrix_sum = confusion_matrix_comb.astype(np.float).sum(axis=1)\nconfusion_matrix_norm = confusion_matrix_comb.astype(np.float)\nconfusion_matrix_norm[0] = confusion_matrix_norm[0] / confusion_matrix_sum[0]\nconfusion_matrix_norm[1] = confusion_matrix_norm[1] / confusion_matrix_sum[1]\ndf = pd.DataFrame(confusion_matrix_norm, index = ['F', 'T'], columns = ['F', 'T'])\nsn.heatmap(df, annot=True) # create a heatmap","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Data with Batch Normalization","metadata":{}},{"cell_type":"code","source":"def train_batch_normalization(X_train, X_val, X_test, y_train, y_val, y_test, learning_rate, epochs):\n  # define model\n  model = define_model_batch_normalization(learning_rate)\n  # define early stopping\n  es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', min_delta=0, patience=5, verbose=1)\n  # fit model\n  history = model.fit(X_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_val, y_val), verbose=1, callbacks=[es])\n  # plot model diagnostics\n  summarize_diagnostics(history)\n  # evaluate model\n  _, accuracy = model.evaluate(X_test, y_test, verbose=1)\n  return model, accuracy","metadata":{"execution":{"iopub.status.busy":"2021-08-16T04:27:00.311031Z","iopub.execute_input":"2021-08-16T04:27:00.311415Z","iopub.status.idle":"2021-08-16T04:27:00.328772Z","shell.execute_reply.started":"2021-08-16T04:27:00.31138Z","shell.execute_reply":"2021-08-16T04:27:00.327761Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, accuracy = train_batch_normalization(X_train, X_val, X_test, y_train, y_val, y_test, 0.001, 75)\nprint(accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T01:15:55.669391Z","iopub.execute_input":"2021-08-15T01:15:55.669739Z","iopub.status.idle":"2021-08-15T05:15:20.86527Z","shell.execute_reply.started":"2021-08-15T01:15:55.669709Z","shell.execute_reply":"2021-08-15T05:15:20.864155Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_test_batch_normalization, accuracy = train_batch_normalization(X_train, X_val, X_test, y_train, y_val, y_test, 0.001, 75)\nprint(accuracy)\ny_pred = model_test_batch_normalization.predict(X_test)\ny_pred_val = y_pred.argmax(axis=1)\ny_test_val = y_test.argmax(axis=1)\ntn, fp, fn, tp = confusion_matrix(y_test_val, y_pred_val).ravel()\nprint(\"{} {} {} {}\".format(tn, fp, fn, tp))\nprint(\"TPR/Recall: {}\".format(tp/(tp+fn)))\nprint(\"TNR/Specificity: {}\".format(tn/(tn+fp)))\nprint(\"PPV/Precision: {}\".format(tp/(tp+fp)))\nprint(\"NPV: {}\".format(tn/(tn+fn)))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T04:27:00.33063Z","iopub.execute_input":"2021-08-16T04:27:00.331091Z","iopub.status.idle":"2021-08-16T07:40:32.986231Z","shell.execute_reply.started":"2021-08-16T04:27:00.331043Z","shell.execute_reply":"2021-08-16T07:40:32.985092Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Data with Data Augmentation","metadata":{}},{"cell_type":"code","source":"def train_data_augmentation(X_train, X_val, X_test, y_train, y_val, y_test, learning_rate, epochs):\n  # define model\n  model = define_model(learning_rate)\n  # create data generator\n  datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n  # prepare iterator\n  it_train = datagen.flow(X_train, y_train, batch_size=64)\n  # define early stopping\n  es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', min_delta=0, patience=5, verbose=1)\n\t# fit model\n  steps = int(X_train.shape[0] / 64)\n  history = model.fit(it_train, epochs=epochs, steps_per_epoch=steps, validation_data=(X_val, y_val), verbose=1, callbacks=[es])\n  # plot model diagnostics\n  summarize_diagnostics(history)\n  # evaluate model\n  _, accuracy = model.evaluate(X_test, y_test, verbose=1)\n  return model, accuracy","metadata":{"execution":{"iopub.status.busy":"2021-08-14T02:57:36.949466Z","iopub.execute_input":"2021-08-14T02:57:36.95022Z","iopub.status.idle":"2021-08-14T02:57:36.978838Z","shell.execute_reply.started":"2021-08-14T02:57:36.950168Z","shell.execute_reply":"2021-08-14T02:57:36.977483Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, accuracy = train_data_augmentation(X_train, X_val, X_test, y_train, y_val, y_test, 0.001, 100)\nprint(accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-08-14T02:57:36.980625Z","iopub.execute_input":"2021-08-14T02:57:36.981097Z","iopub.status.idle":"2021-08-14T07:02:56.136842Z","shell.execute_reply.started":"2021-08-14T02:57:36.981043Z","shell.execute_reply":"2021-08-14T07:02:56.135771Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Data with Data Augmentation and Batch Normalization","metadata":{}},{"cell_type":"code","source":"def train_data_augmentation_batch_normalization(X_train, X_val, X_test, y_train, y_val, y_test, learning_rate, epochs):\n  # define model\n  model = define_model_batch_normalization(learning_rate)\n  # create data generator\n  datagen = ImageDataGenerator(width_shift_range=0.1, height_shift_range=0.1, horizontal_flip=True)\n  # prepare iterator\n  it_train = datagen.flow(X_train, y_train, batch_size=64)\n  # define early stopping\n  es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', min_delta=0, patience=5, verbose=1)\n\t# fit model  \n  steps = int(X_train.shape[0] / 64)\n  history = model.fit(it_train, epochs=epochs, steps_per_epoch=steps, validation_data=(X_val, y_val), verbose=1, callbacks=[es])\n  # plot model diagnostics\n  summarize_diagnostics(history)\n  # evaluate model\n  _, accuracy = model.evaluate(X_test, y_test, verbose=1)\n  return model, accuracy","metadata":{"execution":{"iopub.status.busy":"2021-08-16T14:31:42.827048Z","iopub.execute_input":"2021-08-16T14:31:42.827658Z","iopub.status.idle":"2021-08-16T14:31:42.838642Z","shell.execute_reply.started":"2021-08-16T14:31:42.827617Z","shell.execute_reply":"2021-08-16T14:31:42.837979Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model, accuracy = train_data_augmentation_batch_normalization(X_train, X_val, X_test, y_train, y_val, y_test, 0.001, 100)\nprint(accuracy)","metadata":{"execution":{"iopub.status.busy":"2021-08-15T13:35:28.806872Z","iopub.execute_input":"2021-08-15T13:35:28.807417Z","iopub.status.idle":"2021-08-15T16:38:50.012729Z","shell.execute_reply.started":"2021-08-15T13:35:28.807345Z","shell.execute_reply":"2021-08-15T16:38:50.011745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_test_data_augmentation_batch_normalization, accuracy = train_data_augmentation_batch_normalization(X_train, X_val, X_test, y_train, y_val, y_test, 0.001, 100)\nprint(accuracy)\ny_pred = model_test_data_augmentation_batch_normalization.predict(X_test)\ny_pred_val = y_pred.argmax(axis=1)\ny_test_val = y_test.argmax(axis=1)\ntn, fp, fn, tp = confusion_matrix(y_test_val, y_pred_val).ravel()\nprint(\"{} {} {} {}\".format(tn, fp, fn, tp))\nprint(\"TPR/Recall: {}\".format(tp/(tp+fn)))\nprint(\"TNR/Specificity: {}\".format(tn/(tn+fp)))\nprint(\"PPV/Precision: {}\".format(tp/(tp+fp)))\nprint(\"NPV: {}\".format(tn/(tn+fn)))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T14:31:50.803565Z","iopub.execute_input":"2021-08-16T14:31:50.804031Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Data with Dropout Regularization and Batch Normalization","metadata":{}},{"cell_type":"code","source":"def train_data_dropout_batch(X_train, X_val, X_test, y_train, y_val, y_test, learning_rate, epochs):\n  # define model\n  model = define_model_dropout_regularization_batch_normalization(learning_rate)\n  # define early stopping\n  es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', min_delta=0, patience=5, verbose=1)\n  # fit model\n  history = model.fit(X_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_val, y_val), verbose=1, callbacks=[es])\n  # plot model diagnostics\n  summarize_diagnostics(history)\n  # evaluate model\n  _, accuracy = model.evaluate(X_test, y_test, verbose=1)\n  return model, accuracy","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_test_data_dropout_batch_normalization, accuracy = train_data_dropout_batch(X_train, X_val, X_test, y_train, y_val, y_test, 0.001, 100)\nprint(accuracy)\ny_pred = model_test_data_dropout_batch_normalization.predict(X_test)\ny_pred_val = y_pred.argmax(axis=1)\ny_test_val = y_test.argmax(axis=1)\ntn, fp, fn, tp = confusion_matrix(y_test_val, y_pred_val).ravel()\nprint(\"{} {} {} {}\".format(tn, fp, fn, tp))\nprint(\"TPR/Recall: {}\".format(tp/(tp+fn)))\nprint(\"TNR/Specificity: {}\".format(tn/(tn+fp)))\nprint(\"PPV/Precision: {}\".format(tp/(tp+fp)))\nprint(\"NPV: {}\".format(tn/(tn+fn)))","metadata":{"execution":{"iopub.status.busy":"2021-08-16T17:29:49.283477Z","iopub.execute_input":"2021-08-16T17:29:49.283813Z","iopub.status.idle":"2021-08-16T19:59:00.034766Z","shell.execute_reply.started":"2021-08-16T17:29:49.283782Z","shell.execute_reply":"2021-08-16T19:59:00.033502Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train Data with Weight Regularization and Batch Normalization","metadata":{}},{"cell_type":"code","source":"def train_data_weight_batch(X_train, X_val, X_test, y_train, y_val, y_test, learning_rate, epochs):\n  # define model\n  model = define_model_weight_regularization_batch_normalization(learning_rate)\n  # define early stopping\n  es = keras.callbacks.EarlyStopping(monitor='val_loss', mode='auto', min_delta=0, patience=5, verbose=1)\n  # fit model\n  history = model.fit(X_train, y_train, epochs=epochs, batch_size=64, validation_data=(X_val, y_val), verbose=1, callbacks=[es])\n  # plot model diagnostics\n  summarize_diagnostics(history)\n  # evaluate model\n  _, accuracy = model.evaluate(X_test, y_test, verbose=1)\n  return model, accuracy","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:22:53.383694Z","iopub.execute_input":"2021-08-17T14:22:53.384276Z","iopub.status.idle":"2021-08-17T14:22:53.397172Z","shell.execute_reply.started":"2021-08-17T14:22:53.384229Z","shell.execute_reply":"2021-08-17T14:22:53.396088Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model_test_data_weight_batch_normalization, accuracy = train_data_weight_batch(X_train, X_val, X_test, y_train, y_val, y_test, 0.001, 100)\nprint(accuracy)\ny_pred = model_test_data_weight_batch_normalization.predict(X_test)\ny_pred_val = y_pred.argmax(axis=1)\ny_test_val = y_test.argmax(axis=1)\ntn, fp, fn, tp = confusion_matrix(y_test_val, y_pred_val).ravel()\nprint(\"{} {} {} {}\".format(tn, fp, fn, tp))\nprint(\"TPR/Recall: {}\".format(tp/(tp+fn)))\nprint(\"TNR/Specificity: {}\".format(tn/(tn+fp)))\nprint(\"PPV/Precision: {}\".format(tp/(tp+fp)))\nprint(\"NPV: {}\".format(tn/(tn+fn)))","metadata":{"execution":{"iopub.status.busy":"2021-08-17T14:22:53.39917Z","iopub.execute_input":"2021-08-17T14:22:53.399663Z"},"trusted":true},"execution_count":null,"outputs":[]}]}